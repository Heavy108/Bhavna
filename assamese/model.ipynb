{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"],\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(\n",
    "    txt,\n",
    "    batch_size=4,\n",
    "    max_length=256,\n",
    "    stride=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    eval_freq,\n",
    "    eval_iter,\n",
    "    start_context,\n",
    "    tokenizer,\n",
    "):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculate loss gradients\n",
    "            optimizer.step()  # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                # print(\n",
    "                #     f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                #     f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\"\n",
    "                # )\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded, max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None\n",
    "):\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits\n",
    "            )\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if (\n",
    "            idx_next == eos_id\n",
    "        ):  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda\n",
      "Training on chunk_0.txt\n",
      "----------------\n",
      "তেওঁ এগৰাকী ভাল াাাাাাাাাাাাাাাাাাাাাাাাা\n",
      "তেওঁ এগৰাকী ভাল াাাাাাাাাাাাাাাাাাাাাাাাা\n",
      "তেওঁ এগৰাকী ভাল াাাাাাাাাাাাাাাাাাাাাাাাা\n",
      "তেওঁ এগৰাকী ভাল ািিিািাাািিাাািািাাািিিাা\n",
      "তেওঁ এগৰাকী ভাল ��কৰকৰবাকৰাাাাকৰাাাািকৰ�\n",
      "তেওঁ এগৰাকী ভাল পৰালৰালৰালালৰালালাাালাল\n",
      "তেওঁ এগৰাকী ভাল বানৰ কৰান বৰানৰানৰান বান\n",
      "তেওঁ এগৰাকী ভাল আকৰৰৰাকৰাকৰৰৰৰৰৰৰৰ�\n",
      "তেওঁ এগৰাকী ভাল আগৰাতৰানৰাঁলৰ আঁলানৰ কৰ\n",
      "তেওঁ এগৰাকী ভাল পাই পাইটাইটাইওঁৰণৰাইটাইড�\n",
      "Training on chunk_1.txt\n",
      "----------------\n",
      "তেওঁ এগৰাকী ভাল আছিছিতৰানিতৰৰানৰ আনৰৰা�\n",
      "তেওঁ এগৰাকী ভাল আনৰৰান্য়ানানৰ আনৰানৰৰ\n",
      "তেওঁ এগৰাকী ভাল কৰাইই কলিছিছিছিছিয়ানিছিছ\n",
      "তেওঁ এগৰাকী ভাল কৰানিকৰ কৰৰ কৰৰাই কৰাইই\n",
      "তেওঁ এগৰাকী ভাল আছৰ আছিক আছিছিছৰ আছিছিকৰ �\n",
      "তেওঁ এগৰাকী ভাল কৰালাত কৰালালৰ কাকৰালালা\n",
      "তেওঁ এগৰাকী ভাল কৰানিতে কালৰ কৰ্তিতিকৰ �\n",
      "তেওঁ এগৰাকী ভাল আছৰ্তৰ পৰৰালাত কৰাত পাত�\n",
      "তেওঁ এগৰাকী ভাল আছৰ্তিতিত আগ্ৰ্যৰানিত্\n",
      "তেওঁ এগৰাকী ভাল আৰা বা বিতিতিত্ৰামাতীয় �\n",
      "Training on chunk_10.txt\n",
      "----------------\n",
      "তেওঁ এগৰাকী ভাল আছৰ আলাতিলালালাতৰ আলান্য�\n",
      "তেওঁ এগৰাকী ভাল বিলিলান্তৰ বিকৰ বানৰ বিক�\n",
      "তেওঁ এগৰাকী ভাল বান্য়ান কৰান কৰান কৰা সং\n",
      "তেওঁ এগৰাকী ভাল আছিল। প্ৰাহয় পৰালাত হয়া\n",
      "তেওঁ এগৰাকী ভাল কে বিলৈ আৰু বিলাড়াৰু হয\n",
      "তেওঁ এগৰাকী ভাল আছৰ আছিল। আন্যাই কৰ প্ৰ প\n",
      "তেওঁ এগৰাকী ভাল বিক বাৰ পান বানেয়া হানাময\n",
      "তেওঁ এগৰাকী ভাল কৰা হয়। কৈ কৰা হয় কৰিলা�\n",
      "তেওঁ এগৰাকী ভাল বেওঁৰা বিদৰ বিলৈ বিলাডু �\n",
      "তেওঁ এগৰাকী ভাল আৰু হয়। ইয়াই সামে কেয়া\n",
      "Training on chunk_100.txt\n",
      "----------------\n",
      "তেওঁ এগৰাকী ভাল কৰাই প্ৰালাই ক্ৰালাই কৰ�\n",
      "তেওঁ এগৰাকী ভাল কৰালে প্ৰালৈ পান্য়াকৰ \n",
      "তেওঁ এগৰাকী ভাল কৰে। প্ৰালিম্পান্টা কৰ�\n",
      "তেওঁ এগৰাকী ভাল বাবাবাবলৈছে। তেওঁৰ প্ৰত\n",
      "তেওঁ এগৰাকী ভাল সাধিতে সাধিত সাগ্ৰণ কামূ�\n",
      "তেওঁ এগৰাকী ভাল পৰালীয়া পৰিছিল। এই প্ৰা\n",
      "তেওঁ এগৰাকী ভাল সাম্পতক পৰাষ্টিক পৰালৈ প\n",
      "তেওঁ এগৰাকী ভাল কৰ্ণীয় প্ৰথম প্ৰথম পদক\n",
      "তেওঁ এগৰাকী ভাল বিভাৰতে বিদ্বাডবলৈ বাগৰ�\n",
      "তেওঁ এগৰাকী ভাল পুকালিক পৰালি পদক পৰালীন \n",
      "Training on chunk_101.txt\n",
      "----------------\n",
      "তেওঁ এগৰাকী ভাল কৰিছে। মান্ত্যায়াৰে বা\n",
      "তেওঁ এগৰাকী ভাল কৰিছে। কৰিছিল। বাবেশ কৰি\n",
      "তেওঁ এগৰাকী ভাল কৰিছিল। বাবে বাবে বাবে প�\n",
      "তেওঁ এগৰাকী ভাল আছে বিশ্বাবে আছে। বেয়া �\n",
      "তেওঁ এগৰাকী ভাল কৰিছিল। ক্ত্যালয়াৰ ক্ষ�\n",
      "তেওঁ এগৰাকী ভাল আৰু মানী ব্যালয়াৰ বৰ্ণ\n",
      "তেওঁ এগৰাকী ভাল আৰু পৰাইতেওঁ পৰাম আৰু স�\n",
      "তেওঁ এগৰাকী ভাল কৰিছে। তে সংযাকৰিছিল। তে\n",
      "তেওঁ এগৰাকী ভাল কামতে বাবন্ত্যানী বিবিশ�\n",
      "তেওঁ এগৰাকী ভাল আৰু বাহিচাপে কামৰু পাদিয\n",
      "Training on chunk_102.txt\n",
      "----------------\n",
      "তেওঁ এগৰাকী ভাল আকৰে। তেওঁ প্ৰামৰ পিছত প\n",
      "তেওঁ এগৰাকী ভাল আছে। তেওঁক প্ৰতিয়া কৰিব\n",
      "তেওঁ এগৰাকী ভাল আকৰু বাবে বাবে বাবে বাবে\n",
      "তেওঁ এগৰাকী ভাল আৰু প্ৰতি স্তৰ স্তৰ প্�\n",
      "তেওঁ এগৰাকী ভাল আৰু বাৰ্জন্তিক আছে। এই ত\n",
      "তেওঁ এগৰাকী ভাল আৰু বাসকলে বাসসকলৰ প্ৰত�\n",
      "তেওঁ এগৰাকী ভাল আৰু পৰতে তেওঁ প্ৰতি স্থ\n",
      "তেওঁ এগৰাকী ভাল আৰু বিশ্বাহুত্ৰ পিছ ভাৱ\n",
      "তেওঁ এগৰাকী ভাল আৰু পদ্ৰী স্বাচিত হৈছে \n",
      "তেওঁ এগৰাকী ভাল আৰু পদত কৰে। তেওঁ কবিতে �\n",
      "Training on chunk_103.txt\n",
      "----------------\n",
      "তেওঁ এগৰাকী ভাল আছিল। বাবাদান কৰিছিল। বাব�\n",
      "তেওঁ এগৰাকী ভাল আছিল। তেওঁ বাবে বাবে বাবে\n",
      "তেওঁ এগৰাকী ভাল আৰু বাবে বিশ্বায়ে বাবে \n",
      "তেওঁ এগৰাকী ভাল চাপালিত হয়। তেওঁ সাধাইতে \n",
      "তেওঁ এগৰাকী ভাল আৰু প্ৰকাশিক্ষিণ কৰিছিল\n",
      "তেওঁ এগৰাকী ভাল চলচ্চিত্ৰ বিশ্ববেবৰ বিশ\n",
      "তেওঁ এগৰাকী ভাল চলচ্চিত্ৰতি বিদ্ৰনাথক স�\n",
      "তেওঁ এগৰাকী ভাল আৰু আৰু আছিল। তেওঁ এগতে �\n",
      "তেওঁ এগৰাকী ভাল আৰু পাছত স্তন্যপান কৰা হ�\n",
      "তেওঁ এগৰাকী ভাল কাপে কাপে কামিলিত কৰিছিল।\n",
      "Training on chunk_104.txt\n",
      "----------------\n",
      "তেওঁ এগৰাকী ভাল বিশেষ কামান্ত্ৰত কৰিছিল�\n",
      "তেওঁ এগৰাকী ভাল আছিল। তেওঁ বিভিন্তৰতীয় �\n",
      "তেওঁ এগৰাকী ভাল প্ৰতিবাদিয়াত থাকিব। তেও�\n",
      "তেওঁ এগৰাকী ভাল আৰু স্থান কৰিব। প্ৰথমকা \n",
      "তেওঁ এগৰাকী ভাল আছিল। তেওঁৰ মেকাম আছিল। ত�\n",
      "তেওঁ এগৰাকী ভাল আৰু স্কীয়েচিত্ৰ সাহিত \n",
      "তেওঁ এগৰাকী ভাল হিচাপে গাইছিল। তেওঁ বাঁহু\n",
      "তেওঁ এগৰাকী ভাল হয়। তেওঁ ব্যৱহাৰ কৰা হয�\n",
      "তেওঁ এগৰাকী ভাল প্ৰকৃত্যৱহাৰ কৰা হয়। ত\n",
      "তেওঁ এগৰাকী ভাল চাইছে মেল ফ্ৰাচিত্ৰ বাব�\n",
      "Training on chunk_105.txt\n",
      "----------------\n",
      "তেওঁ এগৰাকী ভাল যায়ত তেওঁ সময়ত তেওঁৰ প্\n",
      "তেওঁ এগৰাকী ভাল আৰু সন্তৰ প্ৰত্বন্তৰ প\n",
      "তেওঁ এগৰাকী ভাল বিচিত্ৰিকাত প্ৰতিষয়ন ক�\n",
      "তেওঁ এগৰাকী ভাল প্ৰতিষ্ঠাপ্ৰদান কৰা হৈ�\n",
      "তেওঁ এগৰাকী ভাল প্ৰথমটোক আৰু সমাজ্যিকল�\n",
      "তেওঁ এগৰাকী ভাল প্ৰতিষ্ঠাপে আৰু কাশীতক \n",
      "তেওঁ এগৰাকী ভাল ভাৰতীয় মহিলাবীকীয় মাজ�\n",
      "তেওঁ এগৰাকী ভাল মহিলা কৰিবলৈ আৰু অভিযান্\n",
      "তেওঁ এগৰাকী ভাল যোগদান কৰা আছিল। তেওঁৰ মা\n",
      "তেওঁ এগৰাকী ভাল পিত্ন অভিনয় কবিতাবাহিত প�\n",
      "Training on chunk_106.txt\n",
      "----------------\n",
      "তেওঁ এগৰাকী ভাল কেইট্ৰিয়াত প্ৰথমিকাশন �\n",
      "তেওঁ এগৰাকী ভাল বিভিন্ন ধৰণৰ বিষয়ববে আছ�\n",
      "তেওঁ এগৰাকী ভাল কামকোমৰ বাবে এক বিজ্ঞান ক\n",
      "তেওঁ এগৰাকী ভাল আৰু স্বাস্থানীতিনীতি বি\n",
      "তেওঁ এগৰাকী ভাল আৰু বিশেষ সময়োচনা কৰা হ�\n",
      "তেওঁ এগৰাকী ভাল আৰু দুটা মান্ধে বিশেষকৈ\n",
      "তেওঁ এগৰাকী ভাল কিছুমান খাদ্য উদ্যা উদ্দ�\n",
      "তেওঁ এগৰাকী ভাল আৰু চলিগে দি শিক্ষা বিভিন\n",
      "তেওঁ এগৰাকী ভাল গোটো স্থানীয়ে বুকী গ্�\n",
      "তেওঁ এগৰাকী ভাল কৰে। মিল আৰু পিত্ত মাজৰ �\n",
      "Training on chunk_107.txt\n",
      "----------------\n",
      "তেওঁ এগৰাকী ভাল কামান বিভিন্ন সময়ৰ বিভিন�\n",
      "তেওঁ এগৰাকী ভাল আৰু তেওঁৰ সময়ত প্ৰতিষ্\n",
      "তেওঁ এগৰাকী ভাল আৰু তাপগতীয় সংগীত পাত্�\n",
      "তেওঁ এগৰাকী ভাল কৰিছিল। ১৯৯৯৯৯৯ চনত চৰ\n",
      "তেওঁ এগৰাকী ভাল কিছতেৰিয়া কৰিছিল। তেওঁ�\n",
      "তেওঁ এগৰাকী ভাল ক্ৰিয়াইছিল আছিল আছিল আগল�\n",
      "তেওঁ এগৰাকী ভাল আছে। তেওঁ অসম চিতৃ প্ৰথম�\n",
      "তেওঁ এগৰাকী ভাল বিয়ান ব্যৱসায়িত হয়।  ২�\n",
      "তেওঁ এগৰাকী ভাল বিদ্যাৰতীয়। তেওঁ ব্ৰী�\n",
      "তেওঁ এগৰাকী ভাল চিন্দুয়াও দেখিব লাভ কৰি \n",
      "Training on chunk_108.txt\n",
      "----------------\n",
      "তেওঁ এগৰাকী ভাল কৰিছিল।   মেঘনাদৰ পৰা হয় \n",
      "তেওঁ এগৰাকী ভাল কৰিছিল। তেওঁ বালি কোনো বা\n",
      "তেওঁ এগৰাকী ভাল কৰে। মেঘনাদে ইন্দ্ৰজিত�\n",
      "তেওঁ এগৰাকী ভাল কৰিছিল।   ১৯০৯  অকিনাৰেং\n",
      "তেওঁ এগৰাকী ভাল কৰিবলৈ আছে।   নাদী লোৱা হ\n",
      "তেওঁ এগৰাকী ভাল কৰিছিল।   ১৯০৯  অকিনাৰী �\n",
      "তেওঁ এগৰাকী ভাল আৰু মৰাণৱৰ বাবে নামৰ বি�\n",
      "তেওঁ এগৰাকী ভাল আৰু অতিনাৱিক অৱ মুদ্ধ মা\n",
      "তেওঁ এগৰাকী ভাল আৰু মাহাত্ৰ কৰাই চৰকাৰ�\n",
      "তেওঁ এগৰাকী ভাল কৰিছে। যেদিনা ইন্দ্ৰজিত\n",
      "Training on chunk_109.txt\n",
      "----------------\n",
      "তেওঁ এগৰাকী ভাল আৰু কৰিছিল। তেওঁৰ পুতো �\n",
      "তেওঁ এগৰাকী ভাল আৰু বিশ্বাস কৰে। এই দ্যা\n",
      "তেওঁ এগৰাকী ভাল হিচাপে আছে। মানুৱে তেওঁ�\n",
      "তেওঁ এগৰাকী ভাল চিন্দিৰটো পৰা চলন কৰিছিল\n",
      "তেওঁ এগৰাকী ভাল আছে। মুঠ সমাপতাননাতিকালৰ \n",
      "তেওঁ এগৰাকী ভাল আৰু প্ৰিয়ালীৰ অনুষ্ঠা\n",
      "তেওঁ এগৰাকী ভাল পছনীয় আৰু শেষ্টাৰ্টী�\n",
      "তেওঁ এগৰাকী ভাল পছনেতাম ছাইলিছ আৰু শেষাগ�\n",
      "তেওঁ এগৰাকী ভাল নি যি আছে। তেওঁ আনন্দিৰট�\n",
      "তেওঁ এগৰাকী ভাল পাইছিল। তাম সম্পনাকৰিছিল।\n",
      "Training on chunk_11.txt\n",
      "----------------\n",
      "তেওঁ এগৰাকী ভাল কৰিছিল, যাতে পৰ্ণগ্ৰাফি \n",
      "তেওঁ এগৰাকী ভাল কৰিছিল। তেওঁ প্ৰথম পৰ্ণ�\n",
      "তেওঁ এগৰাকী ভাল কৰিছিল। কিন্তু আৰু তেওঁ�\n",
      "তেওঁ এগৰাকী ভাল প্ৰকাশিত হৈছে। তেওঁ পিছত\n",
      "তেওঁ এগৰাকী ভাল খায়ৰ চলচ্চিত্ৰ মাজত পৰ�\n",
      "তেওঁ এগৰাকী ভাল প্ৰধমান আৰু সেই সময়ত গা�\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 49\u001b[0m\n\u001b[0;32m     38\u001b[0m     val_loader \u001b[38;5;241m=\u001b[39m create_dataloader_v1(\n\u001b[0;32m     39\u001b[0m         val_data,\n\u001b[0;32m     40\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     46\u001b[0m     )\n\u001b[0;32m     48\u001b[0m     num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m---> 49\u001b[0m     train_losses, val_losses, tokens_seen \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mতেওঁ এগৰাকী ভাল \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 24\u001b[0m, in \u001b[0;36mtrain_model_simple\u001b[1;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[0m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Reset loss gradients from previous batch iteration\u001b[39;00m\n\u001b[0;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m calc_loss_batch(input_batch, target_batch, model, device)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calculate loss gradients\u001b[39;00m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update model weights using loss gradients\u001b[39;00m\n\u001b[0;32m     26\u001b[0m tokens_seen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m input_batch\u001b[38;5;241m.\u001b[39mnumel()\n",
      "File \u001b[1;32md:\\PROJECTS\\Neural_Network\\assamese\\.venv\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PROJECTS\\Neural_Network\\assamese\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PROJECTS\\Neural_Network\\assamese\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device = {device}\")\n",
    "folder_path = \"chunks\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# THE MODEL\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "limit = 50\n",
    "for filename in os.listdir(folder_path):\n",
    "    if limit > 0:\n",
    "        limit -= 1\n",
    "        with open(os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            print(f\"Training on {filename}\\n----------------\")\n",
    "            text_data = f.read()\n",
    "\n",
    "        # Train/validation ratio\n",
    "        train_ratio = 0.90\n",
    "        split_idx = int(train_ratio * len(text_data))\n",
    "        train_data = text_data[:split_idx]\n",
    "        val_data = text_data[split_idx:]\n",
    "\n",
    "        train_loader = create_dataloader_v1(\n",
    "            train_data,\n",
    "            batch_size=2,\n",
    "            max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "            stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        val_loader = create_dataloader_v1(\n",
    "            val_data,\n",
    "            batch_size=2,\n",
    "            max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "            stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "            drop_last=False,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        num_epochs = 10\n",
    "        train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            optimizer,\n",
    "            device,\n",
    "            num_epochs=num_epochs,\n",
    "            eval_freq=5,\n",
    "            eval_iter=5,\n",
    "            start_context=\"তেওঁ এগৰাকী ভাল \",\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
