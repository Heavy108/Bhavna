{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"],\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(\n",
    "    txt,\n",
    "    batch_size=4,\n",
    "    max_length=256,\n",
    "    stride=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    eval_freq,\n",
    "    eval_iter,\n",
    "    start_context,\n",
    "    tokenizer,\n",
    "):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculate loss gradients\n",
    "            optimizer.step()  # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                # print(\n",
    "                #     f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                #     f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\"\n",
    "                # )\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded, max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None\n",
    "):\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits\n",
    "            )\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if (\n",
    "            idx_next == eos_id\n",
    "        ):  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda\n",
      "Training on chunk_0.txt\n",
      "----------------\n",
      "तुम कैसे ्रे क्रर्रर्रे के क्रर्रर्र्\n",
      "तुम कैसे े के के के के के के के के के केरे के\n",
      "तुम कैसे ्र्र्र्र्र्र्र्र्र्र्र्र्\n",
      "तुम कैसे े के के के के के के के के के के के के\n",
      "तुम कैसे े केरेरे के केरे के के के केरे के\n",
      "तुम कैसे ्रेर्रेर९्र्र्र्र्र्र्र्र\n",
      "तुम कैसे े केरेर९ क९े के के के के के के क९९\n",
      "तुम कैसे े के के के के के के के के के के के के\n",
      "तुम कैसे ्क्र्र्र१्र्र्र्र्र्र्तर१\n",
      "तुम कैसे ी अरी सरी कर सी सरी की जिनी मरी सी\n",
      "Training on chunk_1.txt\n",
      "----------------\n",
      "तुम कैसे ेकेरेरेरेरेरेकेक्रेकेर्के\n",
      "तुम कैसे े केर की की की के की की की की की की क\n",
      "तुम कैसे े के के के के के के के के के के के के\n",
      "तुम कैसे ेकेििर म किरेर के केनेर केर्रे\n",
      "तुम कैसे ेकेििक्िििस्िम क केिस्से के क\n",
      "तुम कैसे ेक के मक म मे हेिरक्केििक म म म हे\n",
      "तुम कैसे े कें क्िंके क्रक्क्ं क्िं म्ं\n",
      "तुम कैसे े हे इ सेििमेत स से क हे कू हेसे हे\n",
      "तुम कैसे ेके केकेिके के पे केकेििने ने क\n",
      "तुम कैसे ेके नेर मेिसेस ही हेसेिया हेये �\n",
      "Training on chunk_10.txt\n",
      "----------------\n",
      "तुम कैसे ्र क्र के क्र क्र केर क्र केर के क\n",
      "तुम कैसे ेर के के केर के केर केर केर के के क\n",
      "तुम कैसे ें हें हेरेरेरेर हेन।।ेरेरेर\n",
      "तुम कैसे ें हे हे में हे हें हें हें हे हें\n",
      "तुम कैसे ्ं क्रररं क्ित क्रंत क्त क्ररं\n",
      "तुम कैसे ेरर कें मेररित के केरिरेत केरे\n",
      "तुम कैसे ेर क्र्ररररर्ििर्रिर्रनरर्\n",
      "तुम कैसे  के के कं में के केरे के कें के के क\n",
      "तुम कैसे ० हेें। नें पंें के के केरें के क\n",
      "तुम कैसे  इं क।।े परेरार के कार्की केररे क\n",
      "Training on chunk_100.txt\n",
      "----------------\n",
      "तुम कैसे ४े क्िके क्के क�्क्क्के क्क्क�\n",
      "तुम कैसे े के कें क्र के केर के केर्रर क्र\n",
      "तुम कैसे े के क्ररर क्र्र्र्र क्र्रर्र\n",
      "तुम कैसे े के केनेनां के के के के कािने के क\n",
      "तुम कैसे  इंे क्कें क्र कार केर क्रारार्र �\n",
      "तुम कैसे ०्रार पर्रारराररर०्रारारराररर\n",
      "तुम कैसे ०्ं द्र्ंं्क्र्िके के क्रर्ं\n",
      "तुम कैसे ०े मेनें अन्के मे प्िय्ििनेया �\n",
      "तुम कैसे ००िस्यंकेके कर्सान्िन्केिि�\n",
      "तुम कैसे ०ने ब कन्कर मि मर्त्र के का की के�\n",
      "Training on chunk_1000.txt\n",
      "----------------\n",
      "तुम कैसे ०ेिं केररं क्ररििर क्ररििररि\n",
      "तुम कैसे ०ेंंतकेकेिके के के केितके काि�\n",
      "तुम कैसे ०२२्य की के के हैै।त है केत्तिं\n",
      "तुम कैसे  म२२्य केिकेिकें स्ििकाके केि�\n",
      "तुम कैसे ००२्य स्त स्क्र क�०्र०्ि क्र क�\n",
      "तुम कैसे २ं के स्रें के ह। स्र्रकेिक्रर\n",
      "तुम कैसे २न२्य कें प्र्रि सरित करिका के �\n",
      "तुम कैसे  प२०्य केसत में स सत मे हसर के केक\n",
      "तुम कैसे �  ्री ह। हैंंलिकलिक मी ही हली ह�\n",
      "तुम कैसे �  े ह। क९। हैं हाप्न१ींयनी हैस्\n",
      "Training on chunk_1001.txt\n",
      "----------------\n",
      "तुम कैसे ९े के के के के के के के के के के के क\n",
      "तुम कैसे ९ के केरियार के के के के के के के क�\n",
      "तुम कैसे ११ित के कें के के के कों कें�रिं �\n",
      "तुम कैसे ९ के ज़े के के के मि जीन्याकी की म �\n",
      "तुम कैसे ९ कार कामाम के क्रोिसिका कें कें क\n",
      "तुम कैसे �  का के के के के कात काराल्य का सके ए�\n",
      "तुम कैसे । का हे के के हियत ह्यन के है सं। क�\n",
      "तुम कैसे ९ का के सा है किन का है। कामे है। और य\n",
      "तुम कैसे ९ के संंरामे मिलाममी किययल के मी\n",
      "तुम कैसे ९ के समम मिक का परिनकिया काने हैं �\n",
      "Training on chunk_1002.txt\n",
      "----------------\n",
      "तुम कैसे यंयंंंरिकियंंयंिकरककियंयक\n",
      "तुम कैसे ने के संर्यारिय क्य कर्यात्यार�\n",
      "तुम कैसे ने है है है�ें है है है है है। हैत �\n",
      "तुम कैसे निया है क्या है। इं सकी के के सके ह\n",
      "तुम कैसे निया क्या के है। क्य हैं के क्यंय\n",
      "तुम कैसे ्त मण के सिक मिसकानि के स्र्रीका\n",
      "तुम कैसे ्य मे से सं सता सत्नि के सत मे हैे �\n",
      "तुम कैसे नियह से सके है। इं हैंया मे जनकै �\n",
      "तुम कैसे ुन क्य क्या के सिस�सिस के सका सकार\n",
      "तुम कैसे  मिया के बानिकिल्त्र के स्त्त्य\n",
      "Training on chunk_1003.txt\n",
      "----------------\n",
      "तुम कैसे  केंर्र्र्रे से केंरियंर्र्र\n",
      "तुम कैसे । के स्र्र्ये स्र्र्र्र्र। के\n",
      "तुम कैसे  के के स्र्रिके सं के से सके से स्\n",
      "तुम कैसे १९ारे कें कें केंके के सिके से क�\n",
      "तुम कैसे ्र के के केसके सिके केंगयहाने स�\n",
      "तुम कैसे १९ के के स्रिया है सियों क्र्र क�\n",
      "तुम कैसे १०्यारतिय में प्ये प्रियहारिय\n",
      "तुम कैसे १्यहारार। में का में परी किय है प�\n",
      "तुम कैसे ्र्षव्त्र्त का सकर्र्विय हैस�\n",
      "तुम कैसे १ कार के संलार के से के के करत के के\n",
      "Training on chunk_1004.txt\n",
      "----------------\n",
      "तुम कैसे । के के के क्र्री के कें के के के क\n",
      "तुम कैसे १्र्र्र्र्र्र्रें के के के के\n",
      "तुम कैसे १्री के के के के के के के के के के क\n",
      "तुम कैसे १ेंपर कें के लिक कर्रहार्र्र्�\n",
      "तुम कैसे १्षण क्र्र्रीक क्र कें के के के\n",
      "तुम कैसे १्ष्ष्र्र्रीयो में मे आका कें �\n",
      "तुम कैसे ३ का हे प्री के संचीया गयह कर्री म\n",
      "तुम कैसे ०ेसक क्ष क्टे केलिका मेला हैंखे\n",
      "तुम कैसे  केंत्रिय के लेंपसें के लारींड�\n",
      "तुम कैसे १ींपर में में मेंग्रें मे रीन म\n",
      "Training on chunk_1005.txt\n",
      "----------------\n",
      "तुम कैसे  क्रिके क्रके के क्रिक्रितरिक\n",
      "तुम कैसे  प्रित कें प्रित के परिके क्र्र\n",
      "तुम कैसे  प्रिके स्रक प्रक्रण का प्रिके �\n",
      "तुम कैसे १्रिके ल्रित के सं केंध्रित्र\n",
      "तुम कैसे १्रत्रत्र के स्रन कार कारंर्रत\n",
      "तुम कैसे  प्रके प्री प्र्रिकेस्रमेश्र\n",
      "तुम कैसे  प्रिलें के के लिलारण कां के लिन म\n",
      "तुम कैसे  प्रकारके परकार्रिके क्रके केश\n",
      "तुम कैसे  में पर का के लिलें किल कारिका का क्\n",
      "तुम कैसे  प्रके सेग्रे प्र सारिया से लणी प\n",
      "Training on chunk_1006.txt\n",
      "----------------\n",
      "तुम कैसे १्रे के से के के के के से के के के क\n",
      "तुम कैसे ११११ू के स्रे के के के के से से के\n",
      "तुम कैसे २ु के में के के कें के हुंगी के है\n",
      "तुम कैसे  में में मे में में में मे हैं में\n",
      "तुम कैसे  पर के के बह़िने पर महिले हुछ के क\n",
      "तुम कैसे  प्रों के प्रे प्र के जों के हुरत\n",
      "तुम कैसे  में मे लिए में में महैं में हरुख\n",
      "तुम कैसे  पुछ में गरिलार गत्रते बरीर प्र�\n",
      "तुम कैसे  जलेसेकर करहाथ पर के बुरिलार महा�\n",
      "तुम कैसे  जों खाएके अंगु गड़े बांगें हुछ ह\n",
      "Training on chunk_1007.txt\n",
      "----------------\n",
      "तुम कैसे २्र्र्रिर्रिर्र्र्र्र्र्र\n",
      "तुम कैसे  की है के स्रण्र स्र्र्र्रियार�\n",
      "तुम कैसे  के की के अध्रं कें के अन केंगर्र\n",
      "तुम कैसे  प्र्रे स्रें कें की हैंदी की की\n",
      "तुम कैसे २ुकार। हे प्रूत्रके प्र के लें �\n",
      "तुम कैसे २ की की हैंदिरतिषियने प्र्तिय\n",
      "तुम कैसे  प्र्रत हुर्तर के के हुतथे वरके\n",
      "तुम कैसे  हैं के लियहत हैं। हो हो हुँछत का �\n",
      "तुम कैसे  न्थिरे नहरी पूतरद५ कें के की के\n",
      "तुम कैसे ।कार के लिके करिस्मयत्र क्र प्�\n",
      "Training on chunk_1008.txt\n",
      "----------------\n",
      "तुम कैसे ११्रिके के के की के के के की के के\n",
      "तुम कैसे १्रियहिके सिकिकी के सिकी के स्\n",
      "तुम कैसे ११े अने अने परिक के के अन के अंदि\n",
      "तुम कैसे १२े अने अने अन के अने अने अमेंद्\n",
      "तुम कैसे ११०ियों के परिन के अंदे की के मे\n",
      "तुम कैसे १२े से मेंदे सी सं की स्यनियानी �\n",
      "तुम कैसे २े मेंडे अमींस्त्तंत्तृत्य प\n",
      "तुम कैसे  यांदिक के का करिया कांदर सार्री प�\n",
      "तुम कैसे  बंदे दी संदी सितित के सिय की साथ�\n",
      "तुम कैसे  मिस्ष््ट भाथिलें किके अमनोम उ�\n",
      "Training on chunk_1009.txt\n",
      "----------------\n",
      "तुम कैसे  के के के क्रि की क्य के कर के के के\n",
      "तुम कैसे  के के के के के के के के के पारत के क�\n",
      "तुम कैसे  के लिया है। मे पार्रत मत्रके लिय\n",
      "तुम कैसे  में के के के के के की है। मियों के\n",
      "तुम कैसे  मे लिया है जन किए काल का है की हैं क�\n",
      "तुम कैसे  पुकी से समछल में स्संसंसमछलिय\n",
      "तुम कैसे  ना जाने मेंरे की के की जुरीय के जा�\n",
      "तुम कैसे  ज१े के है के हे के ज्य परे के के जि\n",
      "तुम कैसे  में में मछलब में की में में में क\n",
      "तुम कैसे  है के लियों तुकाद्था जाब किकें त�\n",
      "Training on chunk_101.txt\n",
      "----------------\n",
      "तुम कैसे  है के लियों तुकाद्था जाब किकें त�\n",
      "तुम कैसे  है के लियों तुकाद्था जाब किकें त�\n",
      "तुम कैसे  है के लियों तुकाद्था जाब किकें त�\n",
      "तुम कैसे  है के लियों तुकाद्था जाब किकें त�\n",
      "तुम कैसे  है के लियों तुकाद्था जाब किकें त�\n",
      "तुम कैसे  है के लियों तुकाद्था जाब किकें त�\n",
      "तुम कैसे  है के लियों तुकाद्था जाब किकें त�\n",
      "तुम कैसे  है के लियों तुकाद्था जाब किकें त�\n",
      "तुम कैसे  है के लियों तुकाद्था जाब किकें त�\n",
      "तुम कैसे  है के लियों तुकाद्था जाब किकें त�\n",
      "Training on chunk_1010.txt\n",
      "----------------\n",
      "तुम कैसे  के के बार्य के के के के के के के के �\n",
      "तुम कैसे  के लिया गठिए क के से संग्रके संर�\n",
      "तुम कैसे  के लिया गठिकी के की गठिया गया गठं�\n",
      "तुम कैसे  के क्य किक के के के लिया थांध्तीय\n",
      "तुम कैसे  के ली है। उत्र्र्त उत्त्त्यंग\n",
      "तुम कैसे  भार कियायों काष्त्त्ट्रत्तायत\n",
      "तुम कैसे  भीय कित क्ट्र कियंगारतर्र्र्�\n",
      "तुम कैसे  भीय और क्रारीय का गठबंधा था। के अन\n",
      "तुम कैसे  भीकी विसस्ट्रीयिकार्रतारता ह�\n",
      "तुम कैसे  भीय कि किशिया और से कि सार भाग्वि�\n",
      "Training on chunk_1011.txt\n",
      "----------------\n",
      "तुम कैसे  क्रिया के के की के के के के के की क�\n",
      "तुम कैसे  के क्रं क्र्रियार्र्र्र्र्र�\n",
      "तुम कैसे  के के के के के के के के के के के के क\n",
      "तुम कैसे १ के के कें के प्रें के के केंग्र\n",
      "तुम कैसे  प्रिया करता है। करिया। सरत्र्र �\n",
      "तुम कैसे  प्रिया कियांरिया हैं। सें कारें\n",
      "तुम कैसे  पर्र्रिया गरार के हैंग के सर हैं\n",
      "तुम कैसे  पर्कें के कें कें मेंडी में कें\n",
      "तुम कैसे  क्टिया गाने कें कर्रणाजों कीकरा\n",
      "तुम कैसे  विणार्षणिका प्र्र्र्र्ररणार�\n",
      "Training on chunk_1012.txt\n",
      "----------------\n",
      "तुम कैसे  के स्रत्रित्रित्रत्त के कित्\n",
      "तुम कैसे  के स्रता हैं के हैं के के अलें के �\n",
      "तुम कैसे  के स्र्यों के के की के स्र्ले सम\n",
      "तुम कैसे दें के संदेस के स्रिया था। दे। कर\n",
      "तुम कैसे  कर में की संद्रिया कें को स्रति �\n",
      "तुम कैसे  मुल उलिस में पन्दिया में क्र्र �\n",
      "तुम कैसे  में नित में का में की वबंदी र्र द�\n",
      "तुम कैसे  कर पर्मे में इं मिलियों के का का क\n",
      "तुम कैसे  के तरारी स्त्तादेस्षिया के संद �\n",
      "तुम कैसे  जने के लिया जन के हैं। देवितं से �\n",
      "Training on chunk_1013.txt\n",
      "----------------\n",
      "तुम कैसे  क्रतिक्र के स्रतरी में के के सर\n",
      "तुम कैसे  के स्रारहें के स्या। के समें के स\n",
      "तुम कैसे  के समिक्य के समें की में के संसम\n",
      "तुम कैसे  के प्रारहींधार कारीविक हित्ति �\n",
      "तुम कैसे  के प्रारहियारेस्थांत्रही दें �\n",
      "तुम कैसे  समी परत उन्र्था में जिय कित्र्�\n",
      "तुम कैसे  स्थिक्य के प्थिम्रात्रकालियन\n",
      "तुम कैसे  आदें प्र स्र्ही कियान्या को आदी\n",
      "तुम कैसे  जियान्य में हुई जें जुई कियोंं �\n",
      "तुम कैसे  और के लतियत्ष कर का हैं� के अपरह स\n",
      "Training on chunk_1014.txt\n",
      "----------------\n",
      "तुम कैसे  कियार के किया के के क्यह के कि किय\n",
      "तुम कैसे  कियार के है। इं के है। के के केसे �\n",
      "तुम कैसे  कियार पूर्में के है। है। के के प�\n",
      "तुम कैसे  के में के है। इं में के है। इं के इ\n",
      "तुम कैसे  कि कियाय को के है। हुवर्युवर्य�\n",
      "तुम कैसे  कित तिलिल कर्र्था कितर का गाँवर�\n",
      "तुम कैसे  कित है करियतियोग को गौर उनें आत\n",
      "तुम कैसे  कित तिलवार वार क्तक का था गौर है। व\n",
      "तुम कैसे  कित हुसे आस्वर्या कित हुस्त जार\n",
      "तुम कैसे  के तिलवलिल के के के के है। तिलाल �\n",
      "Training on chunk_1015.txt\n",
      "----------------\n",
      "तुम कैसे १्र्त के के के के में के में में क\n",
      "तुम कैसे १९ के के ले के के के लिक मंदेशिर्\n",
      "तुम कैसे १९ के के के के के के लिक मंदें कें\n",
      "तुम कैसे १९ के के ले के के के लगे में के के ल\n",
      "तुम कैसे  के लारत के के लिए के लार मयके लिए क\n",
      "तुम कैसे १९ के में मं बंदेस समंदेश्रती म\n",
      "तुम कैसे १९ोक पर को के में के के में देश के\n",
      "तुम कैसे १९७० तार्रणा के अप्त अपत्र्तील\n",
      "तुम कैसे १९े  की मीएन कें मेबोरतारी से अप\n",
      "तुम कैसे १९६ारत सिक का थार्रमेत्पंप्रे�\n",
      "Training on chunk_1016.txt\n",
      "----------------\n",
      "तुम कैसे  के पर्रण्र्रण के पर के प्रण्रि\n",
      "तुम कैसे १्तिक के अपश्रकारत के पश्करकार\n",
      "तुम कैसे  के पर्यकी संस्य के प्रत्रमें क\n",
      "तुम कैसे  प्रण्य के परिके पश्रण्रिका पर �\n",
      "तुम कैसे  जित की काजिस्यों की दुनीत करण क�\n",
      "तुम कैसे  के परत के संस्मामिकी संस्कृति �\n",
      "तुम कैसे  में देश्य को दियालें के के जिक द�\n",
      "तुम कैसे  में हों की संसे अधिकर सहों के से\n",
      "तुम कैसे १८ें के अनें सके सके उने अने अस्\n",
      "तुम कैसे ७ में के लर परिकारवर्रवे विक्षा\n",
      "Training on chunk_1017.txt\n",
      "----------------\n",
      "तुम कैसे १्र्रें के कियार्रिया के समें क\n",
      "तुम कैसे १९३ के समें के बने बच्रिकार बच्�\n",
      "तुम कैसे १९६। के बार के अनें के समयह के अध�\n",
      "तुम कैसे १्र्वर्य के से स्तेहते से अने अ\n",
      "तुम कैसे १्तक सित के अने में कार है। बच के �\n",
      "तुम कैसे  के व्यान के से अनें बन का वे अध्य म\n",
      "तुम कैसे  के साथित कित के अन्ध के बन्या के अ\n",
      "तुम कैसे  में स्थ से से अनुषरनें के कार के �\n",
      "तुम कैसे । कियाँ को व्व के अपरो कई अन्गीव�\n",
      "तुम कैसे  तथा कि सी आधन कता ज़ के अन्य सी वसी\n",
      "Training on chunk_1018.txt\n",
      "----------------\n",
      "तुम कैसे  के समें के सर्यह कों के से स्थिक\n",
      "तुम कैसे  तुर्जन के प्रकित के के से से है।\n",
      "तुम कैसे १ के विक्रत की प्र पर की अंती हों\n",
      "तुम कैसे  ती है जिसे पर प्रति के पर के अवर्\n",
      "तुम कैसे  ती है जिले वहाल परा की की लिले जिस\n",
      "तुम कैसे १ के लिक्टक गुलन्रक्टर के ग्ट्\n",
      "तुम कैसे । सापर महालिक्षृष्णिक अवर्जिक\n",
      "तुम कैसे १ स्याने प्रका की स्विश्य गद्य ज\n",
      "तुम कैसे १९८ें समय पर भाग से आवलत्थार्ती\n",
      "तुम कैसे १ के अवश कर प्रवारण एक में ज़वाल्\n",
      "Training on chunk_1019.txt\n",
      "----------------\n",
      "तुम कैसे १ के सिके सिक सीवीक सिक पर स्व के\n",
      "तुम कैसे १ के सिदुरिया के जों के जैन्रन्�\n",
      "तुम कैसे १ के समक संपर सके सिया ज़न के पनी �\n",
      "तुम कैसे १ के लिए जोने सकिए मूड़िया है जि�\n",
      "तुम कैसे १ के लिए जोने जनस्क किलिल मण्ड़\n",
      "तुम कैसे १ के फ़िलेस ऑसकिके समें बनाजाना �\n",
      "तुम कैसे १ को आगिले जैसी जिल का जन कि वर्ड�\n",
      "तुम कैसे १ मिल बिल्टलिए जीनिल्निल की सि\n",
      "तुम कैसे ९ों में दो था। कि नी जित है था। है।\n",
      "तुम कैसे १ को तरनिं और मीली की हैं। हैं आर\n",
      "Training on chunk_102.txt\n",
      "----------------\n",
      "तुम कैसे न्रिकित के किर्र्रिरिर्रिक्\n",
      "तुम कैसे १्र में में मदरखण्रत के महैन के\n",
      "तुम कैसे  में समिर्तिता है। इस्ष्रत्र क�\n",
      "तुम कैसे ० कर समति का में संपर कों सरत्रक �\n",
      "तुम कैसे १ किष्क स्षिक में समत एक महात्र�\n",
      "तुम कैसे  उत्तरतर कार्ति का पर सक्षण के पर\n",
      "तुम कैसे  में प्रतान के प्रमायियान के परि�\n",
      "तुम कैसे  में दिवादाव्यरत की के मस्थात सि�\n",
      "तुम कैसे १ानीषित की की सहै। यह भी दि का एक क\n",
      "तुम कैसे १ान्चित बिष्तिस्नेणी के का का ल�\n",
      "Training on chunk_1020.txt\n",
      "----------------\n",
      "तुम कैसे  केकड़े जो के के कड़े कड़े प्र के\n",
      "तुम कैसे । केर के प्रकेर प्रके के के पर पर\n",
      "तुम कैसे ४र केकें पुरत्रकड़े हैं। हैं।\n",
      "तुम कैसे ४र केके प्रते होते हैं। केकड़ा �\n",
      "तुम कैसे ४र अंति करूप सके के सर के के बदर क\n",
      "तुम कैसे  पर पर स�पनर पर उपने के पेके करने �\n",
      "तुम कैसे  प्रितिष्रत पर प्रविशे केकड़��\n",
      "तुम कैसे  केकड़े सका पे लते हैं काओड़े केक\n",
      "तुम कैसे  केकड़े एक के सक्रियोचलिए जात व�\n",
      "तुम कैसे  केकड़े जोसकत्य करनव्विकमय नव\n",
      "Training on chunk_1021.txt\n",
      "----------------\n",
      "तुम कैसे  की के के के है और प्रिया की के की क�\n",
      "तुम कैसे १११११िकार से सके सकिया जिस्त के\n",
      "तुम कैसे १९ के से के से के से अमें के अपने स\n",
      "तुम कैसे ११११९९ुर्म किस्त से से समदिर्\n",
      "तुम कैसे १८ आनत के असमुस्त पर संग के की सा�\n",
      "तुम कैसे १८ें की समझ लियोत्रत प्रने। मे\n",
      "तुम कैसे १९। अनित आध्त के अलबीत्रगए। जब\n",
      "तुम कैसे ११ अन्त अगसरतिने मान के के काना क�\n",
      "तुम कैसे १८ें समुरोह आत आच्रके अगसर्मा�\n",
      "तुम कैसे  आरत के सब सामानिजेल के लमें के के\n",
      "Training on chunk_1022.txt\n",
      "----------------\n",
      "तुम कैसे  के के के के पूरूप के के प्रन्रें\n",
      "तुम कैसे १९११९९९९ुक्ष्य्र्गर्शन्र्\n",
      "तुम कैसे १९५ में स्थार्शनिकरत्रिक सनी �\n",
      "तुम कैसे १६ेट्रतिष्य में प के प्रत के प्\n",
      "तुम कैसे ६६द में प्रिक संगित हैं हैं। जि\n",
      "तुम कैसे १०र्शन से दार्शनिष्टिटिटिशन �\n",
      "तुम कैसे  भोदर्शना की परस्की नहीं नित इस�\n",
      "तुम कैसे १६्रिक जनों में प्रेनों का जपाम\n",
      "तुम कैसे १६ेट्रहां फ़न पूरल में की पर अन�\n",
      "तुम कैसे  भरद्र के बेड़रेण्डनगाँंजिनत �\n",
      "Training on chunk_1023.txt\n",
      "----------------\n",
      "तुम कैसे  के लिए की के किनहैं। उसक्रनविल\n",
      "तुम कैसे  के परत्रत्रे के के के लिए एक प्र\n",
      "तुम कैसे  के पर के लिए एक करहें के किए और प्\n",
      "तुम कैसे  के पर दिला के के रनहुआ के के के के �\n",
      "तुम कैसे  आदद्रेक्षेनिता है। के को सव्य�\n",
      "तुम कैसे  आदद्वाद करने कार्हो गायोनिश्रा\n",
      "तुम कैसे  आगा हों के प्रसाकर किया तार ग्रग्\n",
      "तुम कैसे १९६िज्माण्चार बोवी मुड्रूगएइ\n",
      "तुम कैसे १९७ में वित्यियों रूरदिशोवेत\n",
      "तुम कैसे  आचान को और ड्रसहीलद एक करती ह�र्\n",
      "Training on chunk_1024.txt\n",
      "----------------\n",
      "तुम कैसे निरित किर्ये के में के नरिकिरं\n",
      "तुम कैसे ने के के रे परतिरें में कित के पर\n",
      "तुम कैसे ने के लिए ते से प्रतिकों के पर्प\n",
      "तुम कैसे  प्रित्ति स्परेजो के प्रियों क\n",
      "तुम कैसे  प्रत्रियों के बीच में भी पर्षत\n",
      "तुम कैसे श दर�ं कुछ से सिद्धानों के प्रे र\n",
      "तुम कैसे १ प्रत्रियोग के भी एक के पतिकार�\n",
      "तुम कैसे  भुर्तानिक प्रिता त्रत्रजिक ब्\n",
      "तुम कैसे  भौत्तिस) के शनलिए किता हुआयांस�\n",
      "तुम कैसे १ प्राओं की विया त्सत कल का पुराधा�\n",
      "Training on chunk_1025.txt\n",
      "----------------\n",
      "तुम कैसे से प्रके सित के से के सकित के से क\n",
      "तुम कैसे  प्रित्यक संगरत्महत्या होत्या\n",
      "तुम कैसे  प्रित्यर्ण के लिए स्थान्रित क�\n",
      "तुम कैसे  प०्वर्य संकरिक को प्रकारिक के �\n",
      "तुम कैसे  भा उन उन्हें के देख्या का सर्टके �\n",
      "तुम कैसे  भों की निक रहांखें उन पर कोवियष�\n",
      "तुम कैसे  भों दाँ शाम निरकार नका नारित करताब\n",
      "तुम कैसे  विषिण से केर्णीग देनसाँखुर्ण�\n",
      "तुम कैसे  विषिण से केकरी न को अक्षियल के न\n",
      "तुम कैसे  छहत्य या नत्यनगरक्रिट्र प्रत�\n",
      "Training on chunk_1026.txt\n",
      "----------------\n",
      "तुम कैसे १६नारी केटुरिलारिके परियर्ट्\n",
      "तुम कैसे  के भूगत के निर्तरहै। किन्है। क\n",
      "तुम कैसे  के बेटिया की के स्थे से बिहाँ निम\n",
      "तुम कैसे  के बाँर्गिँ प्रकार के दौर जनसे म\n",
      "तुम कैसे  के परिया क्विव स्थित राएक रेस्व\n",
      "तुम कैसे  क्थित यह ता एक द्रकाँदर के पर के उ\n",
      "तुम कैसे  क्ववाइं न्है। डुरक दुव जनसीट्�\n",
      "तुम कैसे  क्थित इन्है। इन्या उत्तर्वाल क\n",
      "तुम कैसे १६्रूगर के लाउल में भी देश प्रे�\n",
      "तुम कैसे १७िकरयु रोड के आया जिलाई। इन्हु\n",
      "Training on chunk_1027.txt\n",
      "----------------\n",
      "तुम कैसे क कर वार्वार कि का आत इने देश स्का म\n",
      "तुम कैसे पर किरिक प्रमान प्रेण्रय मारेण\n",
      "तुम कैसे १०ेवार्यों प्रमानिक में जो के क\n",
      "तुम कैसे परिक कोड वें प्रयार के है जनवें �\n",
      "तुम कैसे १६ी इड़िलेज में कि भेत के है जिल\n",
      "तुम कैसे पहलाँंच में के भारत्यारत एक उत्�\n",
      "तुम कैसे पहों कोड वार्य में कोई जिसने था य\n",
      "तुम कैसे पहों को दिके पहै। इस को दृष्माब�\n",
      "तुम कैसे पूरारदेखत्व कैपै किमाने ज परम क\n",
      "तुम कैसे १६ कहुंगया था भारप्रेणीफ जिस्ड �\n",
      "Training on chunk_1028.txt\n",
      "----------------\n",
      "तुम कैसे १ें स्थांग्री के क्षीय के ब्र्�\n",
      "तुम कैसे  पर किष्ट्रेणीफिर के से कार कांग\n",
      "तुम कैसे  पुर के लिए प्रक जबतीय अमें के बा�\n",
      "तुम कैसे  नडिक गया था। के बहर्वर्लेतिया व�\n",
      "तुम कैसे  नडिक गया था। क्षेत्रेणीव्टन्द\n",
      "तुम कैसे  नडल गें स्थां सेल के है। ब्ले के �\n",
      "तुम कैसे  तहों की सुफ्र का पार प्राविविध क�\n",
      "तुम कैसे  नगंरावारा पौरायुव होने के लिए का �\n",
      "तुम कैसे  नडत जात इल एयोगीँ गोँ एक एक बाजे ह\n",
      "तुम कैसे  नडल सांडर्योनी पोरा ईस्थल नखुक\n",
      "Training on chunk_1029.txt\n",
      "----------------\n",
      "तुम कैसे से के पिर्रेणीके सियाक सरकार पि\n",
      "तुम कैसे  के पर के स्वियिक क्षेत्तर्विश\n",
      "तुम कैसे  के पर से सिया था। इस्थित कर स्थान �\n",
      "तुम कैसे  कहु उत्री सर सभा देखें भी मानेतर\n",
      "तुम कैसे  कहु उन्होलिके लिए सदस्थल यक थी\n",
      "तुम कैसे  किया गया मिल के रागाल्ल की से टीत स\n",
      "तुम कैसे  से सिमनी लिए के लिए गई की सदीविश\n",
      "तुम कैसे  मुनी महार उन्हेंडिया। ये। बाग  क\n",
      "तुम कैसे  मुनी महिला मिल के लिहायर गया तक गा\n",
      "तुम कैसे  की समें में रासक्ट्र यह गम़ौिल �\n",
      "Training on chunk_103.txt\n",
      "----------------\n",
      "तुम कैसे सिल के लिए में के के स्कर समयरिक\n",
      "तुम कैसे  के सिक्ष्ट्रेणी के सित के लिए क\n",
      "तुम कैसे  के सिक्स के समें प्राजन के बाद का�\n",
      "तुम कैसे  पहिला पहरा जिन पर के ब्यकिलाइक स�\n",
      "तुम कैसे  नो समें को से के लिए होंने विक ने\n",
      "तुम कैसे  सिंड ने मेंड में प्रा किया था के अ�\n",
      "तुम कैसे  बाद नडी की से गयूसिला गया। बाब्ला�\n",
      "तुम कैसे  सिलाओर्ड पर उन्होन्हें सेलिक �\n",
      "तुम कैसे  डों स्थियोहलेबड हैंम दी। बाद ला\n",
      "तुम कैसे  डों ली वारा है। यह गोबी ता के वह अं�\n",
      "Training on chunk_1030.txt\n",
      "----------------\n",
      "तुम कैसे  की अपन्य के हैं। के हैं सने किया �\n",
      "तुम कैसे  के सिंडिया था तिये हैंबर अपने अप\n",
      "तुम कैसे  की विदोने किले हैं जिसमें के प्\n",
      "तुम कैसे  पर सिंग्रदर्टिकमण्डल सेजिन्\n",
      "तुम कैसे  के अपन्होता हिंदि शित है। जनी व�\n",
      "तुम कैसे ४ फैरी पर लिया ज्य पर हैं। इसल जे�\n",
      "तुम कैसे  के लियों प्रकाशक के हैं। इनचत क�\n",
      "तुम कैसे  में अन्य संजन के समुदिहूम और इश\n",
      "तुम कैसे ५ मों के बल सरकार ये सहर दियाल स्त\n",
      "तुम कैसे  टर्वियों की से लिया थ�। ये हैं। व\n",
      "Training on chunk_1031.txt\n",
      "----------------\n",
      "तुम कैसे  के समें के समने के उनके प्रके के\n",
      "तुम कैसे ४ में में उनका के परिया के से है जि\n",
      "तुम कैसे ४ में में उनकी से पर के समयानी पर�\n",
      "तुम कैसे ४ में भड़ा एक से पत्नी परबार वित क\n",
      "तुम कैसे ४०५ मुखिल्स पहोते हैं अरिया है �\n",
      "तुम कैसे ४०४ में में एक अनन्दविव्यन्य प\n",
      "तुम कैसे ४ में में उनका दिन्हों की लगें इ�\n",
      "तुम कैसे ४. ने का महाराण के लिए स्य की लिए क अ\n",
      "तुम कैसे ४०५१७६१ ;र में इनके में लिए हैं�\n",
      "तुम कैसे २ निले पत्रकारत्रजविल्टन के म�\n",
      "Training on chunk_1032.txt\n",
      "----------------\n",
      "तुम कैसे ८ें सें सरननसके कियके के के प्र\n",
      "तुम कैसे ८ें से परिया ज्यें सिक से पर के प�\n",
      "तुम कैसे ८ें के परिया जाता है। एक परिक है। �\n",
      "तुम कैसे ८७ रतिया है। के उन्हों के लिए के �\n",
      "तुम कैसे ८ेरका जाता कथानवले के वा एक पर मैद�\n",
      "तुम कैसे ४ रहोतारी से नियांवोने की अधिके\n",
      "तुम कैसे ८ें के बहुत की नानंते रीमारत्रि\n",
      "तुम कैसे ८ा निर से ला इमें लिए आए। के लिए के\n",
      "तुम कैसे ८ा नियों के आया गा हिया ता है और फैट �\n",
      "तुम कैसे ८ा जिनके बित नुनमें भी ती है करन�\n",
      "Training on chunk_1033.txt\n",
      "----------------\n",
      "तुम कैसे २ में से से सिमें के से समें से सम\n",
      "तुम कैसे २ के बाद में से से से कार है। इस के अ\n",
      "तुम कैसे ८ें से बाद के सीमान की खाड़ी है जो�\n",
      "तुम कैसे २ के बाद में अनुव के पर बी. के पर के\n",
      "तुम कैसे  बरान बंध से सागरागर के बंगाल की खा�\n",
      "तुम कैसे  महे जनक्ति है। इसकतंत इसके लिए\n",
      "तुम कैसे १००० में से १००१ मनी थे। ये जित �\n",
      "तुम कैसे  महु न्द्य के प्लिन्हें खाड़ी अ�\n",
      "तुम कैसे  महु जनवा से सके ने बढ़े के बहुत था\n",
      "तुम कैसे  महें जिन्न के कुछ प्रत बंविश के\n",
      "Training on chunk_1034.txt\n",
      "----------------\n",
      "तुम कैसे  मियुम्र्में प्री की के की स्थ्\n",
      "तुम कैसे  महि कियों के सम्य के संस्तियों\n",
      "तुम कैसे  महन्ने के में अन्मिन्दुल्लें\n",
      "तुम कैसे  पर के से लमहने कियो जीव्य कर होत\n",
      "तुम कैसे  पंद के मुद के एक न्मलो अनुसिद्ध\n",
      "तुम कैसे  मरक जरी से पृत एक में ग्रापुरल म�\n",
      "तुम कैसे  परि किया मन्द सर में पता के द्ध की\n",
      "तुम कैसे  परक के के मुखुआर्य पर चुक्ट क्र\n",
      "तुम कैसे  परदी एक आपनेजी के अल्ली चीन्या�\n",
      "तुम कैसे १२०८ से भरतथा इंसंत इस के सूबलो�\n",
      "Training on chunk_1035.txt\n",
      "----------------\n",
      "तुम कैसे १२०८ से भरतथा अधी भाषात र्वन प्र�\n",
      "तुम कैसे १२०८ से मनोरी इं भा पर इसु सैन्य �\n",
      "तुम कैसे १२०८ मेंद्ध प्रस्थि कोसीय जीक\n",
      "तुम कैसे १२०८ मेंद्ध एक थी सीमापन्टी मे�\n",
      "तुम कैसे १२ के मेंट के मार्थी पर के काय को ट\n",
      "तुम कैसे १२ के मेंट के मार्थी मार्स का की ह�\n",
      "तुम कैसे १२ के मेंन के दें दर्पोसादिया की\n",
      "तुम कैसे १२ दिया है। महाई के में द्ध कर्थी\n",
      "तुम कैसे १२ दिया है। महाई के में द्ध कर्थी\n",
      "तुम कैसे १२ दिया है। मले के बाद में भाषा की प\n",
      "Training on chunk_1036.txt\n",
      "----------------\n",
      "तुम कैसे २ में उन्हें उन्हें के प्रहें उ\n",
      "तुम कैसे  में अन्य सेन उन्होत्रेन्थ क्त\n",
      "तुम कैसे २ मुद्ध के पहले के पड़ के संगाल क�\n",
      "तुम कैसे १९७५ में एक तामात्तिक्तिकार्त�\n",
      "तुम कैसे २ मुद्यायन का उपनिया का के बाद की स�\n",
      "तुम कैसे १२८ भाष्य उत्तिक पतिक्तन है। इ�\n",
      "तुम कैसे १२८ ५० जैन्देखेत्रम मुखन्मार\n",
      "तुम कैसे १९७ॵि है। यह होते हैं तथ हो जन्त\n",
      "तुम कैसे १९८ (हाय उन्हार फिल्म के जूनित्�\n",
      "तुम कैसे ७ॵर्य हैं। साथ उत्थों का एक म्बन\n",
      "Training on chunk_1037.txt\n",
      "----------------\n",
      "तुम कैसे ११९७११७ मिल में के लिय्यन के नि\n",
      "तुम कैसे १९७७) में के प्रक्षेतार के प्रद\n",
      "तुम कैसे १९७६७ भारतीयू के दिश्रेणीरुक�\n",
      "तुम कैसे १९७९७९न के लिखांगालय६६्राज को �\n",
      "तुम कैसे १९७७ में एक स्टान मेंग अंगोल मे�\n",
      "तुम कैसे १९७९नको लग्रों में परिया नहरो�\n",
      "तुम कैसे १९७ पुर्भात्रत के थें रात्मस बा�\n",
      "तुम कैसे १९७७ पनेर्यिक्टाइल में यूट क्�\n",
      "तुम कैसे १९७७ तरी बान राइ यंबंद लबार्ज़्�\n",
      "तुम कैसे १९७७ पनोंगलने युना एंड क्रस्ट�\n",
      "Training on chunk_1038.txt\n",
      "----------------\n",
      "तुम कैसे १९०००० में से संस्थित के प्रता�\n",
      "तुम कैसे २००१ में समिका प्रतापगढ़ के प्र\n",
      "तुम कैसे २००१ में सई से परतापगढ़ के महिल�\n",
      "तुम कैसे १२०१ जिले ४११ मन५०१५५५ में आन�\n",
      "तुम कैसे १६२०२६३ गईरिक जनी इसिले के दर्\n",
      "तुम कैसे १९६४ थायत रांगों के सिंद रों कई प\n",
      "तुम कैसे १९९९९ जीक हिती है। यहां जिले कि�\n",
      "तुम कैसे ११९१३�०३०१०१ को में पुर स्रताल\n",
      "तुम कैसे ११९९८. प्रताप्रत में अनुस्कतं\n",
      "तुम कैसे १२०० की लेजिलाइपिर में किलेकि�\n",
      "Training on chunk_1039.txt\n",
      "----------------\n",
      "तुम कैसे १२० के के के लिए जिसके के संबंद्\n",
      "तुम कैसे २००० के लिए जीवे से स्थित के लिए\n",
      "तुम कैसे १२०० के लिए जैविक के लिए जाता हैं\n",
      "तुम कैसे १२ के लिए जैविका से के लिए इंग के �\n",
      "तुम कैसे २००० के ले काम कर्निरने कार प्रत\n",
      "तुम कैसे १९८ च्च किया जाता है जो पुरोध के प�\n",
      "तुम कैसे २ का संक्राजन से बाद सारबनफ़्याद �\n",
      "तुम कैसे १ मित करने गया किया होतीन�े और गो�\n",
      "तुम कैसे २ लिए लिए अंग होताविषित्व मियम�\n",
      "तुम कैसे ६ आरोफ़ाइजेन ने में के के अलिए म�\n",
      "Training on chunk_104.txt\n",
      "----------------\n",
      "तुम कैसे २ के स्थान में होते हैं। को के से �\n",
      "तुम कैसे ० में स्थित है। अनुर्वर्मों के\n",
      "तुम कैसे ६िया में स्तर के अध्य के विया गरी\n",
      "तुम कैसे १८ रूप में बनाया जाती हैं। मज्याव\n",
      "तुम कैसे १८ रूप में गया गया गया ग्विया की वि\n",
      "तुम कैसे १९ जातु   में उनके बाज्याने वजबक�\n",
      "तुम कैसे २ मजिना गलों बात होते हैं और बार ।\n",
      "तुम कैसे १३९ू के निया; में निया होती है। ह�\n",
      "तुम कैसे १९ों निरों कॉल्वाल्िया के और मे\n",
      "तुम कैसे १९ों को निया संयना काल पर आचर स्क�\n",
      "Training on chunk_1040.txt\n",
      "----------------\n",
      "तुम कैसे  संबर के से संबरिया के करने के कि�\n",
      "तुम कैसे २०० के से के सेवार्यमें से सेवर �\n",
      "तुम कैसे १९०० के सेवारा में से प्रामाना प्�\n",
      "तुम कैसे १६०११ गले के मुख्यर गुनिकें दि\n",
      "तुम कैसे १६०११ गिक्सीगोलिए गुनिके लिए\n",
      "तुम कैसे १२०११ अमें सेवारा के पीएएक पान०�\n",
      "तुम कैसे २०९० सतीय से अनुमें सिन्देवारा\n",
      "तुम कैसे १� २०९ एक सदी तहाने अलु के अनुए अ�\n",
      "तुम कैसे १ी कैस०० यह कॅधिकोर्षेण�र्षि�\n",
      "तुम कैसे १६०११ मनूसेवा सन्हेलिसरी जीव�\n",
      "Training on chunk_1041.txt\n",
      "----------------\n",
      "तुम कैसे २००००००००००००००००० में में स\n",
      "तुम कैसे २०९०९९९९९९९९९९९९९९९९९९८९९\n",
      "तुम कैसे २०९०३ मैग्वायर के साथ का रहै। अर�\n",
      "तुम कैसे १९९९९९९९९९९९९९९९ में सिंग कई\n",
      "तुम कैसे २००११ मैग्वस्कुट रें सरेजी के\n",
      "तुम कैसे १९००२०००९) सिर अकार मैग के अलाग �\n",
      "तुम कैसे १९९२९८ सकप्रिक न्य में थी। है।\n",
      "तुम कैसे २००३) शेश्रेषिणियर शतिसैक्स �\n",
      "तुम कैसे १९९३ साथ से प्रतीय के अल्म मिल्�\n",
      "तुम कैसे २००३ भाई इवायरी में भापमें सिंग �\n",
      "Training completed in 1818.04 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device = {device}\")\n",
    "folder_path = \"chunks\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# THE MODEL\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "limit = 50\n",
    "for filename in os.listdir(folder_path):\n",
    "    if limit > 0:\n",
    "        limit -= 1\n",
    "        with open(os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            print(f\"Training on {filename}\\n----------------\")\n",
    "            text_data = f.read()\n",
    "\n",
    "        # Train/validation ratio\n",
    "        train_ratio = 0.90\n",
    "        split_idx = int(train_ratio * len(text_data))\n",
    "        train_data = text_data[:split_idx]\n",
    "        val_data = text_data[split_idx:]\n",
    "\n",
    "        train_loader = create_dataloader_v1(\n",
    "            train_data,\n",
    "            batch_size=2,\n",
    "            max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "            stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        val_loader = create_dataloader_v1(\n",
    "            val_data,\n",
    "            batch_size=2,\n",
    "            max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "            stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "            drop_last=False,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        num_epochs = 10\n",
    "        train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            optimizer,\n",
    "            device,\n",
    "            num_epochs=num_epochs,\n",
    "            eval_freq=5,\n",
    "            eval_iter=5,\n",
    "            start_context=\"तुम कैसे \",\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to gpt_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"gpt_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
